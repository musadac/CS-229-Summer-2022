\begin{answer}
\\ \\
\ i. No, Because the learning rate will just jump not converge without regularisation \\ \\
\ ii. No, Updates will become arbitrarily small after a while. \\ \\
iii. No, This will not affect the separability of Data \\ \\
\ iv. Yes, this will penalize large  norm of $\theta$ \ and make data to converge \\ \\
\ \ v. No maybe it will make data totally linearly inseparable \\ \\
\end{answer}
